{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECMWF Data Fetch\n",
        "\n",
        "This study considered nearshore wind and wave conditions by using ERA5 reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMWF) through the Copernicus Climate Data Store. Please run this file using Google Colab\n",
        "\n",
        "**Manual Setup in Google Colab**\n",
        "\n",
        "Some required files must be manually uploaded to Google Colab before running the code. All the necessary files can be found in the current directory's subfolder named `Model_Data`.\n"
      ],
      "metadata": {
        "id": "d6OA7HRRGsEv"
      },
      "id": "d6OA7HRRGsEv"
    },
    {
      "cell_type": "markdown",
      "id": "i3ev_D3WCaXM",
      "metadata": {
        "id": "i3ev_D3WCaXM"
      },
      "source": [
        "ECMWF data must be retrieved on an island-by-island basis. Before running the codes below, please go to the subfolder corresponding to the island under the **Fetch\\_data** directory, and upload the corresponding `shoreline_slope.csv` and `transects_coords.csv` files to the Colab workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"cdsapi>=0.7.4\"\n",
        "!pip install netCDF4\n",
        "!pip install xarray\n",
        "!pip install dask"
      ],
      "metadata": {
        "id": "Q4Rwr_6uJA-e"
      },
      "id": "Q4Rwr_6uJA-e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cdsapi\n",
        "import datetime\n",
        "from scipy.spatial import cKDTree"
      ],
      "metadata": {
        "id": "loM5DYAWI_eZ"
      },
      "id": "loM5DYAWI_eZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V2RT4qC3CMZS",
      "metadata": {
        "id": "V2RT4qC3CMZS"
      },
      "outputs": [],
      "source": [
        "# Transect type options: 'radial' or 'hybrid'\n",
        "transect_types = 'hybrid'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce the number of files that need to be uploaded, the latitude–longitude coordinate ranges corresponding to each island are recorded here."
      ],
      "metadata": {
        "id": "ZGp80w1RJGO9"
      },
      "id": "ZGp80w1RJGO9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FoziS3GdCkPw",
      "metadata": {
        "id": "FoziS3GdCkPw"
      },
      "outputs": [],
      "source": [
        "# # Madhirivaadhoo\n",
        "# sitename = 'Madhirivaadhoo'\n",
        "# lat_min, lat_max = 5.267531, 5.270205\n",
        "# lon_min, lon_max = 73.159007, 73.163432\n",
        "\n",
        "# # Dhakendhoo\n",
        "# sitename = 'Dhakendhoo'\n",
        "# lat_min, lat_max = 5.247097, 5.249889\n",
        "# lon_min, lon_max = 72.890504, 72.896411\n",
        "\n",
        "# # Funadhoo\n",
        "# sitename = 'Funadhoo'\n",
        "# lat_min, lat_max = 5.271686, 5.277585\n",
        "# lon_min, lon_max = 73.029058, 73.037105\n",
        "\n",
        "# sitename = 'Aidhoo'\n",
        "# lat_min, lat_max = 5.185401, 5.189496,\n",
        "# lon_min, lon_max = 73.160561, 73.165986\n",
        "\n",
        "# sitename = 'Mendhoo'\n",
        "# lat_min, lat_max = 5.171016, 5.179060,\n",
        "# lon_min, lon_max = 72.991709, 72.999632\n",
        "\n",
        "sitename = 'Keyodhoo'\n",
        "lat_min, lat_max = 5.274799, 5.278447,\n",
        "lon_min, lon_max = 72.993681, 72.998117\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(\"ecmwf_data\"):\n",
        "    os.makedirs(\"ecmwf_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87713c7",
      "metadata": {
        "id": "d87713c7"
      },
      "source": [
        "Please create a `.cdsapirc` file in the user’s home directory and fill in the API key obtained from your personal application on the ECMWF website.\n",
        "ECMWF website: https://www.ecmwf.int/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1543167c",
      "metadata": {
        "id": "1543167c"
      },
      "outputs": [],
      "source": [
        "cdsapirc_content = \"\"\"\n",
        "url: https://cds.climate.copernicus.eu/api\n",
        "key: e752e5f4-2ec0-4884-b433-ce71651d5f86\n",
        "\"\"\"\n",
        "\n",
        "with open('/root/.cdsapirc', 'w') as f:\n",
        "    f.write(cdsapirc_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00jPVG_xwYIl",
      "metadata": {
        "id": "00jPVG_xwYIl"
      },
      "outputs": [],
      "source": [
        "df_slope = pd.read_csv(\"./shoreline_slope.csv\")\n",
        "df_slope[\"date\"] = pd.to_datetime(df_slope[\"date\"])\n",
        "\n",
        "dates_df = pd.DataFrame({\n",
        "    \"year\": df_slope[\"date\"].dt.year.astype(str),\n",
        "    \"month\": df_slope[\"date\"].dt.month.astype(str).str.zfill(2),\n",
        "    \"day\": df_slope[\"date\"].dt.day.astype(str).str.zfill(2)\n",
        "})\n",
        "\n",
        "dates_df = dates_df.drop_duplicates()\n",
        "grouped = dates_df.groupby([\"year\", \"month\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4MEJKInRx0DW",
      "metadata": {
        "id": "4MEJKInRx0DW"
      },
      "outputs": [],
      "source": [
        "def open_zipped_or_netcdf(file_path, extract_dir=\"temp_extract\"):\n",
        "    \"\"\"\n",
        "    Open a NetCDF file directly or extract and merge from a ZIP archive.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xarray.Dataset\n",
        "        Merged dataset from NetCDF files.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File {file_path} does not exist.\")\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        start_bytes = f.read(4)\n",
        "\n",
        "    if start_bytes.startswith(b\"PK\\x03\\x04\"):\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "        with zipfile.ZipFile(file_path, 'r') as z:\n",
        "            z.extractall(extract_dir)\n",
        "\n",
        "        nc_files = glob.glob(os.path.join(extract_dir, \"**\", \"*.nc\"), recursive=True)\n",
        "        if not nc_files:\n",
        "            raise ValueError(f\"No .nc file found after decompressing {file_path}\")\n",
        "\n",
        "        ds_list = []\n",
        "        for nc in nc_files:\n",
        "            ds_tmp = xr.open_dataset(nc)\n",
        "            ds_list.append(ds_tmp)\n",
        "        ds_merged = xr.merge(ds_list)\n",
        "        return ds_merged\n",
        "\n",
        "    else:\n",
        "        return xr.open_dataset(file_path)\n",
        "\n",
        "\n",
        "def download_gencast_global_two_times(area, day, month, year, hours, out_folder):\n",
        "    \"\"\"\n",
        "    Download ERA5 reanalysis single-level data and save as NetCDF.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str or None\n",
        "        Path to the saved NetCDF file, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    c = cdsapi.Client()\n",
        "\n",
        "    surface_vars = [\n",
        "        'mean_wave_period',\n",
        "        '10m_v_component_of_wind',\n",
        "        '10m_u_component_of_wind',\n",
        "        'significant_height_of_combined_wind_waves_and_swell',\n",
        "    ]\n",
        "\n",
        "    sl_file = f\"single_levels_{year}_{month}.nc\"\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(sl_file):\n",
        "            print(\"=== Downloading ===\")\n",
        "            c.retrieve(\n",
        "                'reanalysis-era5-single-levels',\n",
        "                {\n",
        "                    'product_type': 'reanalysis',\n",
        "                    'format': 'netcdf',\n",
        "                    'variable': surface_vars,\n",
        "                    'year': year,\n",
        "                    'month': month,\n",
        "                    'day': day,\n",
        "                    'time': hours,\n",
        "                    'area': area,\n",
        "                    'grid': [0.25, 0.25],\n",
        "                    'expver': '1',\n",
        "                },\n",
        "                sl_file\n",
        "            )\n",
        "            print(\"Surface data downloaded.\\n\")\n",
        "\n",
        "        ds = open_zipped_or_netcdf(sl_file, extract_dir=\"temp_extract_sl\")\n",
        "\n",
        "        if 'expver' in ds.dims and ds.sizes['expver'] == 1:\n",
        "            ds = ds.isel(expver=0).squeeze('expver')\n",
        "\n",
        "        if 'number' in ds.dims and ds.sizes['number'] == 1:\n",
        "            ds = ds.isel(number=0).squeeze('number')\n",
        "\n",
        "        if 'expver' in ds.coords:\n",
        "            ds = ds.drop_vars('expver')\n",
        "        if 'number' in ds.coords:\n",
        "            ds = ds.drop_vars('number')\n",
        "\n",
        "        if 'valid_time' in ds.dims and 'time' not in ds.dims:\n",
        "            ds = ds.rename({'valid_time': 'time'})\n",
        "\n",
        "        rename_mapping = {\n",
        "            'v10': '10m_v_component_of_wind',\n",
        "            'u10': '10m_u_component_of_wind',\n",
        "            'swh': 'significant_height_of_combined_wind_waves_and_swell',\n",
        "            'mwp': 'mean_wave_period'\n",
        "        }\n",
        "        ds = ds.rename(rename_mapping)\n",
        "\n",
        "        output_file = f\"{out_folder}/{year}_{month}.nc\"\n",
        "        ds.to_netcdf(output_file)\n",
        "        print(f\"\\nFinal file generated: {output_file}\")\n",
        "\n",
        "        return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n=== ERROR ===\")\n",
        "        print(e)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eQGkhzGTx2Ha",
      "metadata": {
        "id": "eQGkhzGTx2Ha"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    target_area = [lat_max, lon_min, lat_min, lon_max]\n",
        "    out_folder = \"ecmwf_data\"\n",
        "\n",
        "    for (year, month), group in grouped:\n",
        "        unique_days = group[\"day\"].unique().tolist()\n",
        "\n",
        "        # Download at Sentinel-2 / Landset default extraction times:\n",
        "        hours = ['05:00']\n",
        "\n",
        "        print(f\"Downloading ECMWF for {year}-{month} days {unique_days}\")\n",
        "\n",
        "        result_file = download_gencast_global_two_times(\n",
        "            target_area,\n",
        "            day=unique_days,\n",
        "            month=month,\n",
        "            year=year,\n",
        "            hours=hours,\n",
        "            out_folder=out_folder\n",
        "        )\n",
        "        if result_file is None:\n",
        "            raise ValueError(f\"Dataset download failed for {year}-{month}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NbEidUtFYN9l",
      "metadata": {
        "id": "NbEidUtFYN9l"
      },
      "outputs": [],
      "source": [
        "df_coords = pd.read_csv(\"./transects_coords.csv\")\n",
        "\n",
        "transect_ids = df_coords[\"transect_id\"].tolist()\n",
        "transect_lons = df_coords[\"longitude\"].values\n",
        "transect_lats = df_coords[\"latitude\"].values\n",
        "\n",
        "nc_files = sorted(glob.glob(\"./ecmwf_data/*.nc\"))\n",
        "records = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LbwbSRhRh26M",
      "metadata": {
        "id": "LbwbSRhRh26M"
      },
      "outputs": [],
      "source": [
        "for nc_path in nc_files:\n",
        "    print(f\"Processing: {nc_path}\")\n",
        "    ds = xr.open_dataset(nc_path)\n",
        "\n",
        "    # Read longitude/latitude grid\n",
        "    lons = ds[\"longitude\"].values\n",
        "    lats = ds[\"latitude\"].values\n",
        "\n",
        "    # Build KDTree\n",
        "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
        "    grid_points = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
        "    tree = cKDTree(grid_points)\n",
        "\n",
        "    # Find the nearest grid point for each transect coordinate\n",
        "    _, idxs = tree.query(np.column_stack([transect_lons, transect_lats]))\n",
        "    lat_idx = idxs // len(lons)\n",
        "    lon_idx = idxs % len(lons)\n",
        "\n",
        "    variables = [\n",
        "        \"10m_v_component_of_wind\",\n",
        "        \"10m_u_component_of_wind\",\n",
        "        \"significant_height_of_combined_wind_waves_and_swell\",\n",
        "        \"mean_wave_period\"\n",
        "    ]\n",
        "\n",
        "    times = ds[\"time\"].values\n",
        "\n",
        "    for t_idx, t in enumerate(times):\n",
        "        date_str = pd.Timestamp(t).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        for i, transect_id in enumerate(transect_ids):\n",
        "            row = {\n",
        "                \"date\": date_str,\n",
        "                \"transect_id\": transect_id,\n",
        "            }\n",
        "\n",
        "            for var in variables:\n",
        "                try:\n",
        "                    val = ds[var].isel(time=t_idx, latitude=lat_idx[i], longitude=lon_idx[i]).values.item()\n",
        "                    row[var] = val\n",
        "                except:\n",
        "                    row[var] = np.nan\n",
        "\n",
        "            records.append(row)\n",
        "\n",
        "    ds.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ALWXfqNh8No",
      "metadata": {
        "id": "5ALWXfqNh8No"
      },
      "outputs": [],
      "source": [
        "df_all = pd.DataFrame(records)\n",
        "df_all.to_csv(f\"./ecmwf_transects_timeseries_{transect_types}.csv\", index=False)\n",
        "\n",
        "print(\"✅ Done! Saved as ./ecmwf_transects_timeseries.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, please place the `ecmwf_transects_timeseries.csv` file generated by running the above code back into the subfolder corresponding to the island under the **Fetch\\_data** directory.\n"
      ],
      "metadata": {
        "id": "FTr2hBLpN0Fs"
      },
      "id": "FTr2hBLpN0Fs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rv7I_M8xu01-",
      "metadata": {
        "id": "Rv7I_M8xu01-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}